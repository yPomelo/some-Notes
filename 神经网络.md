python里的main函数

```python
def main():
    print('this message is from main function')


if __name__ == '__main__':
    main()
```

其中if __name__=="__main__":这个程序块类似与Java和C语言的中main（主）函数

如果没有if __name__ == '__main__'，def main 不会被执行，需要在if __name__ == '__main__'里调用

<img src="C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210414215914054.png" alt="image-20210414215914054" style="zoom:50%;" />

# 神经网络单元的基本计算原理

![image-20210413200028120](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413200028120.png)

S阈值函数指激活函数

**矩阵第一个数字代表行，第二个数字代表列，这是约定。**

![image-20210413202936703](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413202936703.png)

![image-20210413201534716](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413201534716.png)

![image-20210413201554188](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413201554188.png)

损失函数 = 代价函数 

## 梯度下降法

![image-20210413223136706](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413223136706.png)

求导的目的是使损失函数减少，进行梯度下降

## 损失函数

神经网络输出值在与目标值对比之后得到误差，误差进行反向传播来更新网络中的各个单元，但是误差可能会有正负，比如三个节点第一个节点误差为1，第二个为-1，第三个为0，总误差却为0，这不合理。所以要应用损失函数取绝对值或者平方，（目标值-实际值）2  比绝对值更好，因为

- 使用误差的平方，我们可以很容易使用代数计算出梯度下降的斜率。

- 误差函数平滑连续，这使得梯度下降法很好地发挥作用——没有间断，也没有突然的跳跃。

- 越接近最小值，梯度越小，这意味着，如果我们使用这个函数调节步长，超调的风险就会变得较小。

要使用梯度下降法来更新权重，就要计算出误差函数对于权重的斜率

<img src="C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413222141243.png" alt="image-20210413222141243" style="zoom:50%;" />

<img src="C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413222735747.png" alt="image-20210413222735747" style="zoom:50%;" />



描述斜率的表达式

![image-20210413223802118](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413223802118.png)

Ok是第K层的输出，Oj是K层前面的j层

## 学习因子

我们使用学习因子，调节变化，我们可以根据特定的问题，调整这个学习因子。当我们建立线性分类器，作为避免被错误的训练样本拉得太远的一种方式，同时也为了保证权重不会由于持续的超调而在最小值附近来回摆动，我们都发现了这个学习因子。

![image-20210413224148963](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413224148963.png)

符号α是一个因子，这个因子可以调节这些变化的强度，确保不会超调。



矩阵形式的更新权重（省略了学习因子）：

![image-20210413224340983](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413224340983.png)

## 常用的激活函数

### Sigmoid函数

<img src="C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210413224812322.png" alt="image-20210413224812322" style="zoom:50%;" />

因为Sigmoid当x很大的时候变得平坦，所以要让输入值X保持较小的状态，所以使用归一化

一个好的建议是重新调整输入值，将其范围控制在0.0到1.0。输入0会将oj设置为0，这样权重更新表达式就会等于0，从而造成学习能力的丧失，因此在某些情况下，我们会将此输入加上一个小小的偏移，如0.01，避免输入0带来麻烦

### Tanh函数

Tanh是双曲函数中的一个，Tanh()为双曲正切。在数学中，双曲正切“Tanh”是由基本双曲函数双曲正弦和双曲余弦推导而来。公式如下

![img](https://bkimg.cdn.bcebos.com/formula/061b8705cd0dd363c1752fe0d9db0faa.svg)

函数图像如下

[![Tanh函数图像](https://bkimg.cdn.bcebos.com/pic/b64543a98226cffcc6c79651b5014a90f703ea60?x-bce-process=image/resize,m_lfit,w_220,limit_1/format,f_auto)](https://baike.baidu.com/pic/激活函数/2520792/0/b64543a98226cffcc6c79651b5014a90f703ea60?fr=lemma&ct=single)

### ReLU函数

Relu激活函数（The Rectified Linear Unit），用于隐层神经元输出。公式如下

![img](https://bkimg.cdn.bcebos.com/formula/27dc16c40e8ca243c251fe1048fe68a9.svg)

函数图像如下

[![ReLU函数图像](https://bkimg.cdn.bcebos.com/pic/f31fbe096b63f624775e13e08b44ebf81b4ca3d5?x-bce-process=image/resize,m_lfit,w_220,limit_1/format,f_auto)](https://baike.baidu.com/pic/激活函数/2520792/0/f31fbe096b63f624775e13e08b44ebf81b4ca3d5?fr=lemma&ct=single)



## 数据处理

numpy里面的 mean函数 axis=0 表示求每一列平均值 axis = 1表示求每一行平均值。实际上这个axis=0就是选择shape中第一个元素（即第一维）变为1，axis=1就是选择shape中第二个元素变为1。用shape来看会比较方便。

### 标准化：

```python
# 将均值化为0
x -= x.mean( axis = 0)
# 将平方差化为1，这里因为上一步均值变成0，所以x-x.means = x
x /= x.std( axis = 0)
```

numpy里https://www.runoob.com/numpy/numpy-statistical-functions.html

xtd 标准差，var方差，mean均值

## 为模型选择正确的最后一层激活和损失函数

![image-20210416161218461](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210416161218461.png)

# 卷积神经网络

卷 积 的 工 作 原 理： 在 3D 输 入 特 征 图 上 滑 动 （slide） 这 些 3 × 3 或 5 × 5 的 窗 口， 在 每 个 可 能 的 位 置 停 止 并 提 取 周 围 特 征 的 3D 图 块［ 形 状 为 (window_height, window_width, input_depth) ］。 然 后 每 个 3D 图 块 与 学 到 的 同 一 个 权 重 矩 阵［ 叫 作 卷 积 核 （convolution kernel）］ 做 张 量 积， 转 换 成 形 状 为 (output_depth,) 的 1D 向 量。 然 后 对 所 有 这 些 向 量 进 行 空 间 重 组， 使 其 转 换 为 形 状 为 (height, width, output_depth) 的 3D 输 出 特 征 图。 输 出 特 征 图 中 的 每 个 空 间 位 置 都 对 应 于 输 入 特 征 图 中 的 相 同 位 置（ 比 如 输 出 的 右 下 角 包 含 了 输 入 右 下 角 的 信 息）。 举 个 例 子， 利 用 3 × 3 的 窗 口， 向 量 output[ i, j, :] 来 自 3D 图 块 input[ i-1: i + 1, j-1: j + 1, :] 。整 个 过 程 详 见 图 。

![image-20210416203158983](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210416203158983.png)

注 意， 输 出 的 宽 度 和 高 度 可 能 与 输 入 的 宽 度 和 高 度 不 同。 不 同 的 原 因 可 能 有 两 点。 

- 边 界 效 应， 可 以 通 过 对 输 入 特 征 图 进 行 填 充 来 抵 消。 

- 使 用 了 步 幅 （stride）， 稍 后 会 给 出 其 定 义。 我 们 来 深 入 研 究 一 下 这 些 概 念。 

## 1、理 解 边 界 效 应 与 填 充 

假 设 有 一 个 5 × 5 的 特 征 图（ 共 25 个 方 块）。 其 中 只 有 9 个 方 块 可 以 作 为 中 心 放 入 一 个 3 × 3 的 窗 口， 这 9 个 方 块 形 成 一 个 3 × 3 的 网 格（ 见 图 5-5）。 **因 此， 输 出 特 征 图 的 尺 寸 是 3 × 3。** **它 比 输 入 尺 寸 小 了 一 点， 在 本 例 中 沿 着 每 个 维 度 都 正 好 缩 小 了 2 个 方 块。 在 前 一 个 例 子 中 你 也 可 以 看 到 这 种 边 界 效 应 的 作 用： 开 始 的 输 入 尺 寸 为 28 × 28， 经 过 第 一 个 卷 积 层 之 后 尺 寸 变 为 26 × 26。**

![image-20210416203819390](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210416203819390.png)

如 果 你 希 望 输 出 特 征 图 的 空 间 维 度 与 输 入 相 同， 那 么 可 以 使 用 填 充 （padding）。 填 充 是 在 输 入 特 征 图 的 每 一 边 添 加 适 当 数 目 的 行 和 列， 使 得 每 个 输 入 方 块 都 能 作 为 卷 积 窗 口 的 中 心。 对 于 3 × 3 的 窗 口， 在 左 右 各 添 加 一 列， 在 上 下 各 添 加 一 行。 对 于 5 × 5 的 窗 口， 各 添 加 两 行 和 两 列（ 见 图 5-6）。

![image-20210416203948623](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210416203948623.png)

对 于 Conv2D 层， 可 以 通 过 padding 参 数 来 设 置 填 充， 这 个 参 数 有 两 个 取 值：" valid" 表 示 不 使 用 填 充（ 只 使 用 有 效 的 窗 口 位 置）；" same" 表 示“ 填 充 后 输 出 的 宽 度 和 高 度 与 输 入 相 同”。 padding 参 数 的 默 认 值 为 "valid" 。

## 2、理解卷积步幅

影 响 输 出 尺 寸 的 另 一 个 因 素 是 步 幅 的 概 念。 目 前 为 止， 对 卷 积 的 描 述 都 假 设 卷 积 窗 口 的 中 心 方 块 都 是 相 邻 的。 但 两 个 连 续 窗 口 的 距 离 是 卷 积 的 一 个 参 数， 叫 作 步 幅 ，默 认 值 为 1。 也 可 以 使 用 步 进 卷 积 （strided convolution）， 即 步 幅 大 于 1 的 卷 积。 在 图 5-7 中， 你 可 以 看 到 用 步 幅 为 2 的 3 × 3 卷 积 从 5 × 5 输 入 中 提 取 的 图 块（ 无 填 充）。

![image-20210416204138282](C:\Users\yyy\AppData\Roaming\Typora\typora-user-images\image-20210416204138282.png)